{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SparkSession class\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession object\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from CSV file\n",
    "flights = spark.read.csv(\n",
    "    \"data/flights.csv\", sep=\",\", header=True, inferSchema=True, nullValue=\"NA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+----+------+--------+-----+-------+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|org_idx|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+-------+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| NULL|    2.0|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|    0.0|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|    1.0|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|    0.0|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| NULL|    0.0|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "flights = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights).transform(flights)\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+\n",
      "|org|org_idx|    org_dummy|\n",
      "+---+-------+-------------+\n",
      "|ORD|    0.0|(7,[0],[1.0])|\n",
      "|SFO|    1.0|(7,[1],[1.0])|\n",
      "|JFK|    2.0|(7,[2],[1.0])|\n",
      "|LGA|    3.0|(7,[3],[1.0])|\n",
      "|SJC|    4.0|(7,[4],[1.0])|\n",
      "|SMF|    5.0|(7,[5],[1.0])|\n",
      "|TUS|    6.0|(7,[6],[1.0])|\n",
      "|OGG|    7.0|    (7,[],[])|\n",
      "+---+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the one hot encoder class\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Create an instance of the one hot encoder\n",
    "onehot = OneHotEncoder(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# Apply the one hot encoder to the flights data\n",
    "onehot = onehot.fit(flights)\n",
    "flights_onehot = onehot.transform(flights)\n",
    "\n",
    "# Check the results\n",
    "flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+\n",
      "|features                      |delay|\n",
      "+------------------------------+-----+\n",
      "|[11.0,20.0,6.0,2.0,9.48,351.0]|NULL |\n",
      "|[0.0,22.0,2.0,0.0,16.33,82.0] |30   |\n",
      "|[2.0,20.0,4.0,1.0,6.17,82.0]  |-8   |\n",
      "|[9.0,13.0,1.0,0.0,10.33,195.0]|-5   |\n",
      "|[4.0,2.0,5.0,0.0,8.92,65.0]   |NULL |\n",
      "+------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow', \n",
    "    # 'carrier_idx', \n",
    "    'org_idx', \n",
    "    # 'km',\n",
    "    'depart', 'duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights = assembler.transform(flights_onehot)\n",
    "\n",
    "# Check the resulting column\n",
    "flights.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80224\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=43)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = flights_train.count() / flights.count()\n",
    "print(training_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|duration|prediction        |\n",
      "+--------+------------------+\n",
      "|370     |370.0             |\n",
      "|379     |379.0             |\n",
      "|310     |310.0             |\n",
      "|135     |134.99999999999997|\n",
      "|80      |79.99999999999997 |\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.038988885911021e-14"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(flights_test)\n",
    "predictions.select('duration', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.5529772905668064e-14\n",
      "[4.3340831942090705e-16,3.2006956885441626e-16,1.1283273924159106e-15,8.045997111959331e-16,7.274626562625612e-16,1.0000000000000002]\n",
      "4.3340831942090705e-16\n",
      "1.384375825553331e+17\n"
     ]
    }
   ],
   "source": [
    "# Intercept (average minutes on ground)\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Coefficients\n",
    "coefs = regression.coefficients\n",
    "print(coefs)\n",
    "\n",
    "# Average minutes per km\n",
    "minutes_per_km = regression.coefficients[0]\n",
    "print(minutes_per_km)\n",
    "\n",
    "# Average speed in km per hour\n",
    "avg_speed = 60 / minutes_per_km\n",
    "print(avg_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.038988885911021e-14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.384375825553331e+17\n",
      "-5.5529772905668064e-14\n",
      "-5.472517319447213e-14\n",
      "-5.4802310249405505e-14\n"
     ]
    }
   ],
   "source": [
    "# Average speed in km per hour\n",
    "avg_speed_hour = 60 / regression.coefficients[0]\n",
    "print(avg_speed_hour)\n",
    "\n",
    "# Average minutes on ground at OGG\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Average minutes on ground at JFK\n",
    "avg_ground_jfk = inter + regression.coefficients[3]\n",
    "print(avg_ground_jfk)\n",
    "\n",
    "# Average minutes on ground at LGA\n",
    "avg_ground_lga = inter + regression.coefficients[4]\n",
    "print(avg_ground_lga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+----+------+--------+-----+-------+-------------+--------------------+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|org_idx|    org_dummy|            features|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+-------+-------------+--------------------+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| NULL|    2.0|(7,[2],[1.0])|[11.0,20.0,6.0,2....|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|    0.0|(7,[0],[1.0])|[0.0,22.0,2.0,0.0...|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|    1.0|(7,[1],[1.0])|[2.0,20.0,4.0,1.0...|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|    0.0|(7,[0],[1.0])|[9.0,13.0,1.0,0.0...|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| NULL|    0.0|(7,[0],[1.0])|[4.0,2.0,5.0,0.0,...|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+-------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|depart|depart_bucket|\n",
      "+------+-------------+\n",
      "|  9.48|          3.0|\n",
      "| 16.33|          5.0|\n",
      "|  6.17|          2.0|\n",
      "| 10.33|          3.0|\n",
      "|  8.92|          2.0|\n",
      "+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-------------+-------------+\n",
      "|depart|depart_bucket| depart_dummy|\n",
      "+------+-------------+-------------+\n",
      "|  9.48|          3.0|(7,[3],[1.0])|\n",
      "| 16.33|          5.0|(7,[5],[1.0])|\n",
      "|  6.17|          2.0|(7,[2],[1.0])|\n",
      "| 10.33|          3.0|(7,[3],[1.0])|\n",
      "|  8.92|          2.0|(7,[2],[1.0])|\n",
      "+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer, OneHotEncoder\n",
    "\n",
    "# Create buckets at 3 hour intervals through the day\n",
    "buckets = Bucketizer(splits=[0, 3, 6, 9, 12, 15, 18, 21, 24], inputCol='depart', outputCol='depart_bucket')\n",
    "\n",
    "# Bucket the departure times\n",
    "bucketed = buckets.transform(flights)\n",
    "bucketed.select('depart', 'depart_bucket').show(5)\n",
    "\n",
    "# Create a one-hot encoder\n",
    "onehot = OneHotEncoder(inputCols=['depart_bucket'], outputCols=['depart_dummy'])\n",
    "\n",
    "# One-hot encode the bucketed departure times\n",
    "flights_onehot = onehot.fit(bucketed).transform(bucketed)\n",
    "flights_onehot.select('depart', 'depart_bucket', 'depart_dummy').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the RMSE on testing data\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# rmse = RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
    "# print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# # Average minutes on ground at OGG for flights departing between 21:00 and 24:00\n",
    "# avg_eve_ogg = regression.intercept\n",
    "# print(avg_eve_ogg)\n",
    "\n",
    "# # Average minutes on ground at OGG for flights departing between 03:00 and 06:00\n",
    "# avg_night_ogg = regression.intercept + regression.coefficients[9]\n",
    "# print(avg_night_ogg)\n",
    "\n",
    "# # Average minutes on ground at JFK for flights departing between 03:00 and 06:00\n",
    "# avg_night_jfk = regression.intercept + regression.coefficients[9] + regression.coefficients[3]\n",
    "# print(avg_night_jfk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is 2.038988885911021e-14\n",
      "[4.3340831942090705e-16,3.2006956885441626e-16,1.1283273924159106e-15,8.045997111959331e-16,7.274626562625612e-16,1.0000000000000002]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Fit linear regression model to training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Make predictions on testing data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is 0.9974271621644267\n",
      "[0.0,0.0,0.0,0.0,0.0,0.9885175118869292]\n",
      "Number of coefficients equal to 0: 5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Fit Lasso model (λ = 1, α = 1) to training data\n",
    "regression = LinearRegression(labelCol='duration', regParam=1, elasticNetParam=1).fit(flights_train)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol='duration').evaluate(regression.transform(flights_test))\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)\n",
    "\n",
    "# Number of zero coefficients\n",
    "zero_coeff = sum([beta == 0 for beta in regression.coefficients])\n",
    "print(\"Number of coefficients equal to 0:\", zero_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big-data-with-pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
