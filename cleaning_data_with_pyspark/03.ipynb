{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ExampleApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "departures_df = spark.read.format('csv').options(Header=True).load('../data/AA_DFW_2014_Departures_Short.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 157198 rows took 6.182705 seconds\n",
      "Counting 157198 rows again took 1.149794 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n",
      "Removing departures_df from cache\n",
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the full and split files into DataFrames\n",
    "# full_df = spark.read.csv('departures_full.txt.gz')\n",
    "# split_df = spark.read.csv('departures_0*.txt.gz')\n",
    "\n",
    "# # Print the count and run time for each DataFrame\n",
    "# start_time_a = time.time()\n",
    "# print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "# print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "# start_time_b = time.time()\n",
    "# print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "# print(\"Time to run: %f\" % (time.time() - start_time_b))\n",
    "\n",
    "# import split is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ExampleApp\n",
      "Driver TCP port: 65435\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition count before change: 2\n",
      "Partition count after change: 3\n"
     ]
    }
   ],
   "source": [
    "# Store the number of partitions in variable\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "\n",
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
    "\n",
    "# Recreate the DataFrame using the departures data file\n",
    "departures_df = spark.read.csv('../data/AA_DFW_2014_Departures_Short.csv.gz').distinct()\n",
    "\n",
    "# Print the number of partitions for each instance\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Join the flights_df and aiports_df DataFrames\n",
    "# normal_df = flights_df.join(airports_df, \\\n",
    "#     flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# # Show the query plan\n",
    "# normal_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# # Count the number of rows in the normal DataFrame\n",
    "# normal_count = normal_df.count()\n",
    "# normal_duration = time.time() - start_time\n",
    "\n",
    "# start_time = time.time()\n",
    "# # Count the number of rows in the broadcast DataFrame\n",
    "# broadcast_count = broadcast_df.count()\n",
    "# broadcast_duration = time.time() - start_time\n",
    "\n",
    "# # Print the counts and the duration of the tests\n",
    "# print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "# print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))\n",
    "\n",
    "# Normal count:\t\t119910\tduration: 0.626342\n",
    "# Broadcast count:\t119910\tduration: 0.309472"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big-data-with-pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
