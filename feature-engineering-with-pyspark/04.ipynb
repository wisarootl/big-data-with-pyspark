{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark: SparkSession = SparkSession.builder.appName(\"ExampleApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"data/2017_StPaul_MN_Real_Estate.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df = df.withColumn('offmarketdate', to_date('offmarketdate', 'M/d/yyyy H:mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def train_test_split_date(df, split_col, test_days=45):\n",
    "  \"\"\"Calculate the date to split test and training sets\"\"\"\n",
    "  # Find how many days our data spans\n",
    "  max_date = df.agg({split_col: 'max'}).collect()[0][0]\n",
    "  min_date = df.agg({split_col: 'min'}).collect()[0][0]\n",
    "  # Subtract an integer number of days from the last date in dataset\n",
    "  split_date = max_date - timedelta(days=test_days)\n",
    "  return split_date\n",
    "\n",
    "# Find the date to use in spitting test and train\n",
    "split_date = train_test_split_date(df, 'offmarketdate')\n",
    "\n",
    "# Create Sequential Test and Training Sets\n",
    "train_df = df.where(df['offmarketdate'] < split_date)\n",
    "test_df = df.where(df['offmarketdate'] >= split_date).where(df['LISTDATE'] <= split_date) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('LISTDATE', to_date('LISTDATE', 'M/d/yyyy H:mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------------------+------------+\n",
      "|  LISTDATE|offmarketdate|DAYSONMARKET_Original|DAYSONMARKET|\n",
      "+----------+-------------+---------------------+------------+\n",
      "|2017-10-06|   2018-01-24|                  110|          65|\n",
      "|2017-09-18|   2017-12-12|                   82|          83|\n",
      "|2017-11-07|   2017-12-12|                   35|          33|\n",
      "|2017-10-30|   2017-12-11|                   42|          41|\n",
      "|2017-07-14|   2017-12-19|                  158|         149|\n",
      "|2017-10-25|   2017-12-20|                   45|          46|\n",
      "|2017-12-07|   2017-12-23|                   16|           3|\n",
      "|2017-11-22|   2017-12-16|                   24|          18|\n",
      "|2017-10-27|   2017-12-13|                   47|          44|\n",
      "|2017-09-29|   2017-12-12|                   12|          72|\n",
      "|2017-11-28|   2017-12-11|                   13|          12|\n",
      "|2017-09-09|   2018-01-17|                  119|          92|\n",
      "|2017-11-18|   2017-12-15|                   26|          22|\n",
      "|2017-12-07|   2017-12-18|                   11|           3|\n",
      "|2017-11-25|   2018-01-02|                   38|          15|\n",
      "|2017-11-09|   2018-01-03|                   55|          31|\n",
      "|2017-10-18|   2017-12-26|                   69|          53|\n",
      "|2017-10-03|   2017-12-15|                   40|          68|\n",
      "|2017-10-16|   2017-12-15|                   60|          55|\n",
      "|2017-11-18|   2017-12-28|                   40|          22|\n",
      "+----------+-------------+---------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, to_date, lit\n",
    "\n",
    "split_date = to_date(lit('2017-12-10'))\n",
    "# Create Sequential Test set\n",
    "test_df = df.where(df['offmarketdate'] >= split_date).where(df['LISTDATE'] <= split_date)\n",
    "\n",
    "# Create a copy of DAYSONMARKET to review later\n",
    "test_df = test_df.withColumn('DAYSONMARKET_Original', test_df['DAYSONMARKET'])\n",
    "\n",
    "# Recalculate DAYSONMARKET from what we know on our split date\n",
    "test_df = test_df.withColumn('DAYSONMARKET', datediff(split_date, 'LISTDATE'))\n",
    "\n",
    "# Review the difference\n",
    "test_df[['LISTDATE', 'offmarketdate', 'DAYSONMARKET_Original', 'DAYSONMARKET']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop column with low observation\n",
    "\n",
    "# obs_threshold = 30\n",
    "# cols_to_remove = list()\n",
    "\n",
    "# # Inspect first 10 binary columns in list\n",
    "# for col in binary_cols[0:10]:\n",
    "#   # Count the number of 1 values in the binary column\n",
    "#   obs_count = df.agg({col:'sum'}).collect()[0][0]\n",
    "#   # If less than our observation threshold, remove\n",
    "#   if obs_count <= obs_threshold:\n",
    "#     cols_to_remove.append(col)\n",
    "    \n",
    "# # Drop columns and print starting and ending dataframe shapes\n",
    "# new_df = df.drop(*cols_to_remove)\n",
    "\n",
    "# print('Rows: ' + str(df.count()) + ' Columns: ' + str(len(df.columns)))\n",
    "# print('Rows: ' + str(new_df.count()) + ' Columns: ' + str(len(new_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naively Handling Missing and Categorical Values\n",
    "\n",
    "# # Replace missing values\n",
    "# df = df.fillna(-1, subset=['WALKSCORE', 'BIKESCORE'])\n",
    "\n",
    "# # Create list of StringIndexers using list comprehension\n",
    "# indexers = [StringIndexer(inputCol=col, outputCol=col+\"_IDX\")\\\n",
    "#             .setHandleInvalid(\"keep\") for col in categorical_cols]\n",
    "# # Create pipeline of indexers\n",
    "# indexer_pipeline = Pipeline(stages=indexers)\n",
    "# # Fit and Transform the pipeline to the original data\n",
    "# df_indexed = indexer_pipeline.fit(df).transform(df)\n",
    "\n",
    "# # Clean up redundant columns\n",
    "# df_indexed = df_indexed.drop(*categorical_cols)\n",
    "# # Inspect data transformations\n",
    "# print(df_indexed.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# # Train a Gradient Boosted Trees (GBT) model.\n",
    "# gbt = GBTRegressor(featuresCol=\"features\",\n",
    "#                            labelCol=\"SALESCLOSEPRICE\",\n",
    "#                            predictionCol=\"Prediction_Price\",\n",
    "#                            seed=42\n",
    "#                            )\n",
    "\n",
    "# # Train model.\n",
    "# model = gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# # Select columns to compute test error\n",
    "# evaluator = RegressionEvaluator(labelCol=\"SALESCLOSEPRICE\", \n",
    "#                                 predictionCol=\"Prediction_Price\")\n",
    "# # Dictionary of model predictions to loop over\n",
    "# models = {'Gradient Boosted Trees': gbt_predictions, 'Random Forest Regression': rfr_predictions}\n",
    "# for key, preds in models.items():\n",
    "#   # Create evaluation metrics\n",
    "#   rmse = evaluator.evaluate(preds, {evaluator.metricName: \"rmse\"})\n",
    "#   r2 = evaluator.evaluate(preds, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "#   # Print Model Metrics\n",
    "#   print(key + ' RMSE: ' + str(rmse))\n",
    "#   print(key + ' R^2: ' + str(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting Results\n",
    "# NOTE: The array of feature importances, importances has already been created for you from model.featureImportances.toArray()\n",
    "\n",
    "# # Convert feature importances to a pandas column\n",
    "# fi_df = pd.DataFrame(importances, columns=['importance'])\n",
    "\n",
    "# # Convert list of feature names to pandas column\n",
    "# fi_df['feature'] = pd.Series(feature_cols)\n",
    "\n",
    "# # Sort the data based on feature importance\n",
    "# fi_df.sort_values(by=['importance'], ascending=False, inplace=True)\n",
    "\n",
    "# # Inspect Results\n",
    "# fi_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving & Loading Models\n",
    "\n",
    "\n",
    "# from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "# # Save model\n",
    "# model.save('rfr_no_listprice')\n",
    "\n",
    "# # Load model\n",
    "# loaded_model = RandomForestRegressionModel.load('rfr_no_listprice')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big-data-with-pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
